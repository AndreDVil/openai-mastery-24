{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9362ec2",
   "metadata": {},
   "source": [
    "# üß† Project 00 ‚Äì Foundations  \n",
    "\n",
    "This notebook introduces the fundamental concepts of the OpenAI Python SDK, how models are called, how responses are structured, and how core parameters influence the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5569f66",
   "metadata": {},
   "source": [
    "## üîπ **Block 1 ‚Äî Imports, Environment Variables, and Client Setup**\n",
    "\n",
    "In this block we:\n",
    "\n",
    "1. Import essential packages (`os`, `time`, `dotenv`, `openai`)\n",
    "2. Load environment variables from `.env`\n",
    "3. Retrieve `OPENAI_API_KEY`\n",
    "4. Create the OpenAI client using `OpenAI(api_key=...)`\n",
    "\n",
    "### Why this matters\n",
    "- Every project in this repository will require a correctly configured OpenAI client.\n",
    "- Using `.env` files provides security and prevents hard-coding API keys.\n",
    "- Understanding this step ensures that you can debug API authentication issues in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env loaded: True\n",
      "OpenAI client created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Project 00 ‚Äì Foundations\n",
    "# Cell 1: Imports and basic configuration\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env in the project root\n",
    "# If your .env is in the repository root and this notebook is inside projects/00-foundations,\n",
    "# we go one level up:\n",
    "dotenv_loaded = load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "print(f\".env loaded: {dotenv_loaded}\")\n",
    "\n",
    "# Read API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. \"\n",
    "        \"Create a .env file in the repository root with OPENAI_API_KEY=your_key_here.\"\n",
    "    )\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"OpenAI client created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91409528",
   "metadata": {},
   "source": [
    "## üîπ Block 2 ‚Äî First Call Using the Responses API\n",
    "\n",
    "This block performs the first generation request:\n",
    "\n",
    "```python\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Say hello in a friendly way.\"\n",
    ")\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "- `responses.create` is the modern OpenAI endpoint for free-form text.\n",
    "- `response.output_text` is a convenient way to extract the generated text.\n",
    "- Measuring latency shows the real performance characteristics.\n",
    "- The response object also contains important metadata such as:\n",
    "  - token usage\n",
    "  - model information\n",
    "  - configuration parameters\n",
    "  - structured output components\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- This request pattern is foundational for all later projects:\n",
    "  - RAG (Retrieval-Augmented Generation)\n",
    "  - multi-agent systems\n",
    "  - planning modules\n",
    "  - tool-calling agents\n",
    "  - vision/audio pipelines\n",
    "  - autonomous systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cbb9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Response Output ===\n",
      "\n",
      "Hello there! Hope you‚Äôre having a wonderful day! üòä\n",
      "\n",
      "=======================\n",
      "\n",
      "Latency: 1.041 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0268e30d9373826a0069395614a2b88193b37ce8fc07934be0', created_at=1765365268.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_0268e30d9373826a0069395614e7308193a71b36baf0c4644c', content=[ResponseOutputText(annotations=[], text='Hello there! Hope you‚Äôre having a wonderful day! üòä', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=14, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=27), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project 00 ‚Äì Foundations\n",
    "# Cell 2: First \"Hello, OpenAI\" call using the Responses API\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",        # Fast, cheap, great for experiments\n",
    "    input=\"Say hello in a friendly way.\"\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(\"=== Response Output ===\\n\")\n",
    "print(response.output_text)  # Most convenient way to extract text\n",
    "print(\"\\n=======================\\n\")\n",
    "\n",
    "print(f\"Latency: {elapsed:.3f} seconds\")\n",
    "\n",
    "# Explore raw response\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71e4f8",
   "metadata": {},
   "source": [
    "## üîπ Block 3 ‚Äî Exploring Generation Parameters (`temperature` and `top_p`)\n",
    "\n",
    "In this block we examined how different parameter settings influence output style.\n",
    "\n",
    "### temperature\n",
    "\n",
    "Controls randomness:\n",
    "\n",
    "- 0.0 = deterministic and stable\n",
    "- 1.0 = balanced and natural\n",
    "- 1.5 = more creative and expressive\n",
    "\n",
    "### top_p\n",
    "\n",
    "Controls sampling diversity:\n",
    "\n",
    "- 1.0 = model considers all possible tokens\n",
    "- lower values = restrict output to higher-probability words\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- These parameters strongly affect creativity, factuality and consistency.\n",
    "- They are essential in controlling agent behavior.\n",
    "- They help tune performance for different application needs (creative writing vs. factual precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc52d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- temperature=0.0, top_p=1.0 ---\n",
      "A mystical genie swirls within the bottle, waiting to grant three wishes to the one who dares to uncork it.\n",
      "(Latency: 1.667s)\n",
      "\n",
      "--- temperature=1.0, top_p=1.0 ---\n",
      "A shimmering genie swirls within the confines of an ornate bottle, waiting for a wish to be set free.\n",
      "(Latency: 1.290s)\n",
      "\n",
      "--- temperature=2.0, top_p=1.0 ---\n",
      "A mysterious genie swirls within an ancient glass bottle, patiently awaiting his next magic summon.\n",
      "(Latency: 1.234s)\n",
      "\n",
      "--- temperature=0.7, top_p=0.5 ---\n",
      "A shimmering genie swirls within the bottle, waiting to grant wishes with a mischievous grin.\n",
      "(Latency: 0.931s)\n"
     ]
    }
   ],
   "source": [
    "# Project 00 ‚Äì Foundations\n",
    "# Cell 3: Exploring temperature and top_p\n",
    "\n",
    "prompt = \"Write one short sentence describing a genie in a bottle.\"\n",
    "\n",
    "def test_params(temp, top_p):\n",
    "    start = time.time()\n",
    "    r = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=prompt,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\n--- temperature={temp}, top_p={top_p} ---\")\n",
    "    print(r.output_text)\n",
    "    print(f\"(Latency: {elapsed:.3f}s)\")\n",
    "\n",
    "\n",
    "# Run different configurations\n",
    "test_params(0.0, 1.0)   # deterministic\n",
    "test_params(1.0, 1.0)   # creative\n",
    "test_params(1,5, 1.0)   # very creative\n",
    "test_params(0.7, 0.5)   # controlled diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70384960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== top_p = 1.0 ==========\n",
      "\n",
      "Sample 1:\n",
      "Opaque, ethereal, tantalizing, weathered, enigmatic.\n",
      "\n",
      "Sample 2:\n",
      "Veiled, entrancing, ethereal, whispering, concealed\n",
      "\n",
      "Sample 3:\n",
      "Veiled, ominous, enchanting, eldritch, cryptic.\n",
      "\n",
      "========== top_p = 0.5 ==========\n",
      "\n",
      "Sample 1:\n",
      "Enigmatic, shadowy, ancient, ethereal, whispering\n",
      "\n",
      "Sample 2:\n",
      "Enigmatic, shadowy, ancient, alluring, whispering\n",
      "\n",
      "Sample 3:\n",
      "Enigmatic, shadowy, ornate, whispering, ancient\n",
      "\n",
      "========== top_p = 0.1 ==========\n",
      "\n",
      "Sample 1:\n",
      "Enigmatic, shadowy, ancient, ethereal, cryptic\n",
      "\n",
      "Sample 2:\n",
      "Enigmatic, shadowy, ancient, whispering, iridescent\n",
      "\n",
      "Sample 3:\n",
      "Enigmatic, shadowy, ancient, whispering, iridescent\n"
     ]
    }
   ],
   "source": [
    "# Stronger demonstration of top_p: multiple adjectives per output\n",
    "\n",
    "prompt = \"Generate five different creative adjectives to describe a mysterious door. Output only the adjectives, separated by commas.\"\n",
    "\n",
    "def test_top_p_multi(top_p):\n",
    "    print(f\"\\n========== top_p = {top_p} ==========\")\n",
    "    for i in range(3):\n",
    "        r = client.responses.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            input=prompt,\n",
    "            temperature=1.2,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(r.output_text)\n",
    "\n",
    "# Run tests\n",
    "test_top_p_multi(1.0)\n",
    "test_top_p_multi(0.5)\n",
    "test_top_p_multi(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6d635",
   "metadata": {},
   "source": [
    "## Mini Summary ‚Äî Temperature vs Top-P\n",
    "\n",
    "### Temperature (randomness)\n",
    "- Controls how **spread out** the probability distribution becomes.\n",
    "- **Higher temperature (1.0‚Äì1.5)** ‚Üí more creative, varied, surprising.\n",
    "- **Lower temperature (0.0‚Äì0.3)** ‚Üí more stable, predictable, deterministic.\n",
    "- Acts like a *global chaos factor* for the model‚Äôs sampling.\n",
    "\n",
    "### Top-P (nucleus sampling)\n",
    "- Limits the model to only the **top portion of cumulative probability**.\n",
    "- **top_p = 1.0** ‚Üí full freedom (all tokens available).\n",
    "- **top_p = 0.5** ‚Üí only the tokens whose total probability accumulates to 50%.\n",
    "- **top_p = 0.1** ‚Üí extremely restrictive, often yielding repetitive patterns.\n",
    "\n",
    "### How they interact\n",
    "- Temperature spreads or sharpens the distribution.\n",
    "- Top-P *cuts off* the tail of unlikely tokens.\n",
    "- When the most probable token dominates the distribution,\n",
    "  **lowering top-p has little visible effect** (the model simply picks the same token).\n",
    "\n",
    "### When effects are most visible\n",
    "- When the model must choose multiple tokens (lists, sentences).\n",
    "- When several tokens have similar probabilities.\n",
    "- When temperature is not too low (e.g., ‚â• 0.8).\n",
    "- When outputs involve creativity rather than strict logic.\n",
    "\n",
    "### Practical rule of thumb\n",
    "- **Use temperature** to control creativity.\n",
    "- **Use top-p** to control diversity.\n",
    "- In many real systems:\n",
    "  - If you want **stable, factual answers** ‚Üí `temperature=0, top_p=1`.\n",
    "  - If you want **creative brainstorming** ‚Üí `temperature=1.0, top_p between 0.8‚Äì1.0`.\n",
    "  - If you want **controlled creativity** ‚Üí `temperature=0.7, top_p=0.5`.\n",
    "\n",
    "This understanding becomes essential when tuning RAG pipelines, agents, and any system requiring reproducible or stylistically consistent outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf780349",
   "metadata": {},
   "source": [
    "## üîπ Block 4 ‚Äî Comparing Models (Speed, Style, and Behavior)\n",
    "\n",
    "In this block we compare two or more OpenAI models side by side in order to build intuition about:\n",
    "\n",
    "1. **Latency** (how fast each model responds)  \n",
    "2. **Style** (how rich, coherent, or creative the text feels)  \n",
    "3. **Token usage** (how many tokens each model tends to generate)  \n",
    "4. **Practical trade-offs** between small, fast models and larger, smarter ones  \n",
    "\n",
    "This skill is essential for real-world AI engineering, because choosing the right model can reduce cost, improve UX, and increase system stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Why comparing models matters\n",
    "\n",
    "Different models behave differently even when given the exact same prompt:\n",
    "\n",
    "### Smaller models (e.g., gpt-4o-mini)\n",
    "- Fast  \n",
    "- Cheap  \n",
    "- Great for high-volume or real-time tasks  \n",
    "- Sometimes simpler or less nuanced responses  \n",
    "\n",
    "### Larger or more capable models (e.g., gpt-4.1 or gpt-4o)\n",
    "- More detailed, coherent, and context-aware  \n",
    "- Better reasoning  \n",
    "- Higher quality writing  \n",
    "- Slightly slower  \n",
    "- Higher token cost  \n",
    "\n",
    "Understanding these differences helps you design systems that balance:\n",
    "\n",
    "- Quality  \n",
    "- Speed  \n",
    "- Cost  \n",
    "- Predictability  \n",
    "- User experience  \n",
    "\n",
    "---\n",
    "\n",
    "## What to observe during this comparison\n",
    "\n",
    "When running the test, pay attention to:\n",
    "\n",
    "### 1. **Response time**\n",
    "- Does one model consistently respond faster?\n",
    "\n",
    "### 2. **Output quality**\n",
    "- Does one model produce richer or more coherent descriptions?\n",
    "- Are there noticeable differences in vocabulary or style?\n",
    "\n",
    "### 3. **Token usage**\n",
    "- Larger models sometimes use more tokens for the same task.\n",
    "- This affects cost, especially at scale.\n",
    "\n",
    "### 4. **Consistency**\n",
    "- Some models generate more stable outputs across repeated runs.\n",
    "- This matters for structured tasks and agents.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this block is foundational\n",
    "\n",
    "Later in the 24-project journey, you will:\n",
    "\n",
    "- Tune agents that pick models dynamically  \n",
    "- Optimize system cost and latency  \n",
    "- Run RAG pipelines where speed matters  \n",
    "- Build autonomous systems where consistency is critical  \n",
    "- Use specialized models (vision, audio, multimodal reasoning)  \n",
    "\n",
    "For all of these, you must understand that **model selection is a design decision**, not a fixed choice.\n",
    "\n",
    "This block teaches exactly how to compare them in practice.\n",
    "\n",
    "---\n",
    "\n",
    "Continue to the code block to run the actual model comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0149f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Model: gpt-4o-mini\n",
      "==============================\n",
      "\n",
      "--- Output ---\n",
      "Nestled among snow-draped pines, the cozy cabin exudes warmth, its wooden beams glowing softly in the flicker of a crackling fire. Frosted windows frame the world outside, where snowflakes dance like delicate whispers, while inside, the scent of pine and cinnamon wraps around you like a cherished blanket. A hand-knit throw lies invitingly on the worn leather couch, and the gentle hum of a kettle brewing tea adds to the serene ambiance. Here, time slows, and a sense of peace envelops you, as the outside chill fades away, leaving only the comfort of companionship and the promise of quiet moments.\n",
      "\n",
      "Latency: 4.094 seconds\n",
      "Tokens - input: 34, output: 128, total: 162\n",
      "\n",
      "==============================\n",
      "Model: gpt-4o\n",
      "==============================\n",
      "\n",
      "--- Output ---\n",
      "Nestled among snow-draped pines, the cozy cabin exudes warmth with its glowing windows casting a golden hue onto the pristine white landscape. Inside, the crackling fireplace fills the room with a comforting warmth, while the scent of pine and cinnamon lingers in the air. Soft woolen blankets are draped over a plush sofa, inviting moments of quiet reflection and contentment. The gentle sound of snowflakes tapping against the windows creates a serene, cocoon-like atmosphere that wraps around you like a cherished memory.\n",
      "\n",
      "Latency: 2.969 seconds\n",
      "Tokens - input: 34, output: 106, total: 140\n",
      "\n",
      "==============================\n",
      "Model: gpt-4.1-mini\n",
      "==============================\n",
      "\n",
      "--- Output ---\n",
      "Nestled among snow-laden pines, the cozy cabin radiates warmth, its wooden walls glowing softly from the crackling fireplace inside. The scent of pine and burning wood mingles with the crisp, frosty air seeping through the windowpanes, where delicate frost patterns glisten in the pale winter light. Inside, plush blankets and flickering candles invite quiet moments of reflection, filling the space with a serene, comforting embrace. It‚Äôs a haven of peace, where time slows and the outside world feels beautifully distant.\n",
      "\n",
      "Latency: 2.877 seconds\n",
      "Tokens - input: 34, output: 107, total: 141\n",
      "\n",
      "\n",
      "=== Summary Table ===\n",
      "Model: gpt-4o-mini\n",
      "  Latency: 4.094s\n",
      "  Tokens  - input: 34, output: 128, total: 162\n",
      "\n",
      "Model: gpt-4o\n",
      "  Latency: 2.969s\n",
      "  Tokens  - input: 34, output: 106, total: 140\n",
      "\n",
      "Model: gpt-4.1-mini\n",
      "  Latency: 2.877s\n",
      "  Tokens  - input: 34, output: 107, total: 141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Project 00 ‚Äì Foundations\n",
    "# Cell 4: Comparing models (latency, style, tokens)\n",
    "\n",
    "models_to_test = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1-mini\",  # if this fails, you can comment/remove this and/or use \"gpt-4.1\"\n",
    "]\n",
    "\n",
    "comparison_prompt = (\n",
    "    \"In 3‚Äì4 sentences, describe a cozy cabin in the mountains during winter, \"\n",
    "    \"focusing on atmosphere, small details, and emotions.\"\n",
    ")\n",
    "\n",
    "def compare_models(models, prompt, temperature=0.7):\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        start = time.time()\n",
    "        try:\n",
    "            r = client.responses.create(\n",
    "                model=model,\n",
    "                input=prompt,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling model {model}: {e}\")\n",
    "            continue\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        # Extract text and usage (tokens)\n",
    "        text = r.output_text\n",
    "        usage = getattr(r, \"usage\", None)\n",
    "\n",
    "        print(f\"\\n--- Output ---\\n{text}\\n\")\n",
    "        print(f\"Latency: {elapsed:.3f} seconds\")\n",
    "\n",
    "        if usage:\n",
    "            print(\n",
    "                f\"Tokens - input: {usage.input_tokens}, \"\n",
    "                f\"output: {usage.output_tokens}, \"\n",
    "                f\"total: {usage.total_tokens}\"\n",
    "            )\n",
    "            results.append(\n",
    "                {\n",
    "                    \"model\": model,\n",
    "                    \"latency\": elapsed,\n",
    "                    \"input_tokens\": usage.input_tokens,\n",
    "                    \"output_tokens\": usage.output_tokens,\n",
    "                    \"total_tokens\": usage.total_tokens,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(\"No usage information available for this response.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = compare_models(models_to_test, comparison_prompt, temperature=0.7)\n",
    "\n",
    "print(\"\\n\\n=== Summary Table ===\")\n",
    "for item in results:\n",
    "    print(\n",
    "        f\"Model: {item['model']}\\n\"\n",
    "        f\"  Latency: {item['latency']:.3f}s\\n\"\n",
    "        f\"  Tokens  - input: {item['input_tokens']}, \"\n",
    "        f\"output: {item['output_tokens']}, total: {item['total_tokens']}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46d4bf",
   "metadata": {},
   "source": [
    "## Analysis ‚Äì Model Comparison (gpt-4o-mini vs gpt-4o vs gpt-4.1-mini)\n",
    "\n",
    "From the previous block, we observed:\n",
    "\n",
    "### Latency and tokens\n",
    "\n",
    "- `gpt-4o-mini`: ~4.1s, 162 tokens (34 in, 128 out)  \n",
    "- `gpt-4o`: ~3.0s, 140 tokens (34 in, 106 out)  \n",
    "- `gpt-4.1-mini`: ~2.9s, 141 tokens (34 in, 107 out)\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- In a single run, the ‚Äúmini‚Äù model was not the fastest.  \n",
    "  - Network variance and model warm-up can dominate latency in isolated tests.  \n",
    "- `gpt-4o-mini` produced the **longest output**, which increases both cost and time.  \n",
    "\n",
    "### Style and quality\n",
    "\n",
    "- All three models produced high-quality, coherent descriptions.  \n",
    "- `gpt-4o-mini`:\n",
    "  - More verbose and highly sensory\n",
    "  - Slightly more repetitive in expressing comfort and warmth  \n",
    "- `gpt-4o`:\n",
    "  - More concise and balanced\n",
    "  - Feels editorial and polished  \n",
    "- `gpt-4.1-mini`:\n",
    "  - Similar to gpt-4o, slightly more formal\n",
    "  - Strong imagery without becoming overly long  \n",
    "\n",
    "### Practical interpretation\n",
    "\n",
    "- For this type of descriptive writing, all models are ‚Äúgood enough‚Äù.  \n",
    "- The main differences are:\n",
    "  - length of the response  \n",
    "  - subtle stylistic preferences  \n",
    "  - small latency variations  \n",
    "\n",
    "This block shows that **model choice is not only about raw ‚Äúintelligence‚Äù**, but about the trade-off between:\n",
    "\n",
    "- quality  \n",
    "- cost  \n",
    "- latency  \n",
    "- verbosity  \n",
    "\n",
    "In future projects, this intuition will be important when choosing which model to use for:\n",
    "- agents  \n",
    "- RAG pipelines  \n",
    "- user-facing UI responses  \n",
    "- high-volume workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdaa1c",
   "metadata": {},
   "source": [
    "## üîπBlock 5 - Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6c2065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cost Estimates (USD) ===\n",
      "\n",
      "Model: gpt-4o-mini\n",
      "  Price per 1M tokens: $0.15\n",
      "  Input tokens:  34  ‚Üí cost: $0.00000510\n",
      "  Output tokens: 128 ‚Üí cost: $0.00001920\n",
      "  Total cost:                  ‚Üí $0.00002430\n",
      "\n",
      "\n",
      "Model: gpt-4o\n",
      "  Price per 1M tokens: $5.0\n",
      "  Input tokens:  34  ‚Üí cost: $0.00017000\n",
      "  Output tokens: 106 ‚Üí cost: $0.00053000\n",
      "  Total cost:                  ‚Üí $0.00070000\n",
      "\n",
      "\n",
      "Model: gpt-4.1-mini\n",
      "  Price per 1M tokens: $0.2\n",
      "  Input tokens:  34  ‚Üí cost: $0.00000680\n",
      "  Output tokens: 107 ‚Üí cost: $0.00002140\n",
      "  Total cost:                  ‚Üí $0.00002820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Project 00 ‚Äì Foundations\n",
    "# Cell 5: Cost estimation per model call\n",
    "\n",
    "# Tabela de pre√ßos aproximados (USD) por 1 milh√£o de tokens\n",
    "# Ajuste conforme necess√°rio.\n",
    "\n",
    "prices_per_million = {\n",
    "    \"gpt-4o-mini\": 0.15,   # $0.15 / 1M tokens\n",
    "    \"gpt-4o\": 5.00,        # $5.00 / 1M tokens\n",
    "    \"gpt-4.1-mini\": 0.20,  # $0.20 / 1M tokens\n",
    "    \"gpt-4.1\": 4.00,       # $4.00 / 1M tokens (n√£o usado agora, mas deixado pronto)\n",
    "    # voc√™ pode adicionar muitos outros aqui depois\n",
    "}\n",
    "\n",
    "def estimate_cost(model, input_tokens, output_tokens, price_table):\n",
    "\n",
    "    if model not in price_table:\n",
    "        raise ValueError(f\"Model '{model}' not found in price table.\")\n",
    "\n",
    "    price_per_million = price_table[model]\n",
    "\n",
    "    # custo = tokens * (pre√ßo por 1M / 1,000,000)\n",
    "    cost_input  = (input_tokens  / 1_000_000) * price_per_million\n",
    "    cost_output = (output_tokens / 1_000_000) * price_per_million\n",
    "    cost_total  = cost_input + cost_output\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"price_per_million\": price_per_million,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cost_input_usd\": cost_input,\n",
    "        \"cost_output_usd\": cost_output,\n",
    "        \"cost_total_usd\": cost_total,\n",
    "    }\n",
    "\n",
    "\n",
    "# Agora usamos os \"results\" gerados no Bloco 4\n",
    "cost_results = []\n",
    "\n",
    "for r in results:   # results veio do bloco 4\n",
    "    cost_info = estimate_cost(\n",
    "        model=r[\"model\"],\n",
    "        input_tokens=r[\"input_tokens\"],\n",
    "        output_tokens=r[\"output_tokens\"],\n",
    "        price_table=prices_per_million\n",
    "    )\n",
    "    cost_results.append(cost_info)\n",
    "\n",
    "# Mostrar os resultados\n",
    "print(\"\\n=== Cost Estimates (USD) ===\")\n",
    "for c in cost_results:\n",
    "    print(\n",
    "        f\"\\nModel: {c['model']}\\n\"\n",
    "        f\"  Price per 1M tokens: ${c['price_per_million']}\\n\"\n",
    "        f\"  Input tokens:  {c['input_tokens']}  ‚Üí cost: ${c['cost_input_usd']:.8f}\\n\"\n",
    "        f\"  Output tokens: {c['output_tokens']} ‚Üí cost: ${c['cost_output_usd']:.8f}\\n\"\n",
    "        f\"  Total cost:                  ‚Üí ${c['cost_total_usd']:.8f}\\n\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
